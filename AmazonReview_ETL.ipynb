{"nbformat_minor": 1, "cells": [{"source": "# Sentiment Analysis and Rating Prediction From The Review Text", "cell_type": "markdown", "metadata": {}}, {"source": "Sentiment ananlysis and rating prediction are among the imporant machine learning topics that help companies find if the users are happy or unhappy with the service/product provided. The users write reviews of the products/services on various platforms, such as social networking websites like Facebook and Twitter, Blogs, and service offering websites. The ananlysis of such reviews to find the coustomer satisfaction will be helpful for companies to improve their products as well as the customer service.\n\nIn this project, I aim to build a machine learning system that will predict the user rating from his text review. Precisely, I will work on building the models for the following.\n\n1. Predict the users' sentiments (positive or negative).\n2. Predict his product/service rating on a scale of 1 to 5.\n\nIn the following, I describe the data set I have used for the purpose, detail the ETL performed and explain the deep learning model employed.", "cell_type": "markdown", "metadata": {}}, {"source": "## The Data Set", "cell_type": "markdown", "metadata": {}}, {"source": "To build and test our model for the project, I decided to work with Amazon's Bokks review data set available freely at http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz. The data set contains about 8.9 million instances where the user wrote reviews of the books and provided their ratings. The general structure of the review is as follows.", "cell_type": "markdown", "metadata": {}}, {"source": "{\n\n  \"reviewerID\": \"A2SUAM1J3GNN3B\",\n  \"asin\": \"0000013714\",\n  \"reviewerName\": \"J. McDonald\",\n  \"helpful\": [2, 3],\n  \"reviewText\": \"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!\",\n  \"overall\": 5.0,\n  \"summary\": \"Heavenly Highway Hymns\",\n  \"unixReviewTime\": 1252800000,\n  \"reviewTime\": \"09 13, 2009\"\n}\n", "cell_type": "raw", "metadata": {}}, {"source": "## Import the required libraries", "cell_type": "markdown", "metadata": {}}, {"source": "# !pip uninstall keras --yes\n# !pip install keras==2.1.2 ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 2}, {"source": "import numpy as np\nimport pandas as pd\nimport gzip\nimport glob\nimport os\nimport re", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 3}, {"source": "## ETL", "cell_type": "markdown", "metadata": {}}, {"source": "Let us first download the data set", "cell_type": "markdown", "metadata": {}}, {"source": "!wget -O reviews_Books.json.gz http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "The second step is to parse the data set into a data frame.", "cell_type": "markdown", "metadata": {}}, {"source": "def parse(path):\n  g = gzip.open(path, 'rb')\n  for l in g:\n    yield eval(l)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 6}, {"source": "Due to the limitations of the Free Tier of IBM Cloud dataplatform, I was getting the memory error while trying to read all the contents and load the data in data frames. So I did manually in 6 steps while restarting my kernel every time after each step. In each step, I saved the loaded data into a csv file. This gave me a total of six csv files.\n1. AmazonBooksReviews1.csv\n2. AmazonBooksReviews2.csv\n3. AmazonBooksReviews3.csv\n4. AmazonBooksReviews4.csv\n5. AmazonBooksReviews5.csv\n6. AmazonBooksReviews6.csv", "cell_type": "markdown", "metadata": {}}, {"source": "def getDF(path, startIndex,recordCount):\n  \n  i = 0\n  counter = 0    \n  df = {}\n  for d in parse(path):\n    if counter<startIndex:\n        counter += 1\n        continue\n    df[i] = d\n    i += 1\n    print(i)\n    if i+startIndex>=recordCount:\n        break\n  return pd.DataFrame.from_dict(df, orient='index')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 7}, {"source": "df = getDF('reviews_Books.json.gz',7900001,8900000)\ndf.to_csv('AmazonBooksReviews6.csv')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "The next step is to read all the scv files into data frame", "cell_type": "markdown", "metadata": {}}, {"source": "path =os.getcwd() \nallFiles = glob.glob(path + \"/*.csv\")\nallFiles.sort()\nallFiles", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "['/gpfs/global_fs01/sym_shared/YPProdSpark/user/s551-0e069dd8b32632-b79326335986/notebook/work/AmazonBooksReviews1.csv',\n '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s551-0e069dd8b32632-b79326335986/notebook/work/AmazonBooksReviews2.csv',\n '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s551-0e069dd8b32632-b79326335986/notebook/work/AmazonBooksReviews3.csv',\n '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s551-0e069dd8b32632-b79326335986/notebook/work/AmazonBooksReviews4.csv',\n '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s551-0e069dd8b32632-b79326335986/notebook/work/AmazonBooksReviews5.csv',\n '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s551-0e069dd8b32632-b79326335986/notebook/work/AmazonBooksReviews6.csv']"}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "list_ = []\n\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=0)\n    list_.append(df)\n\ndf_complete = pd.concat(list_, axis = 0, ignore_index = True)\nprint(df_complete.shape)\nprint(df_complete.columns)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "(8898036, 10)\nIndex(['Unnamed: 0', 'helpful', 'reviewText', 'overall', 'reviewerName',\n       'unixReviewTime', 'reviewerID', 'asin', 'reviewTime', 'summary'],\n      dtype='object')\n"}], "execution_count": 9}, {"source": "Next, we get rid of all the columns except those having reviews (reviewText) and rataings (overall)", "cell_type": "markdown", "metadata": {}}, {"source": "df_ReviewRating = df_complete[['reviewText','overall']]\ndf_ReviewRating.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(8898036, 2)"}, "execution_count": 10, "metadata": {}}], "execution_count": 10}, {"source": "Let us see the distribution of ratings in the data set", "cell_type": "markdown", "metadata": {}}, {"source": "df_ReviewRating.groupby('overall').count()/df_ReviewRating.shape[0]*100", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "         reviewText\noverall            \n1.0        3.639208\n2.0        4.665108\n3.0       10.734571\n4.0       24.982951\n5.0       55.971520", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n    </tr>\n    <tr>\n      <th>overall</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>3.639208</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>4.665108</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>10.734571</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>24.982951</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>55.971520</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 13, "metadata": {}}], "execution_count": 13}, {"source": "print(df_ReviewRating['reviewText'].apply(str).map(len).max())", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "32658\n"}], "execution_count": 11}, {"source": "The dataframe has a review with the lenght as much as 32658 characters. Allowing maximum review length as much exhausts the Free tiers resources. Therefore, I decided to keep the reviews with a maximum length of 500.", "cell_type": "markdown", "metadata": {}}, {"source": "#Filter the reviews with lenght of more than 200 charecters\ndf_ReviewRatingFiltered = df_ReviewRating[df_ReviewRating['reviewText'].apply(str).map(len)<500]\ndf_ReviewRatingFiltered = df_ReviewRatingFiltered[df_ReviewRatingFiltered['reviewText'].apply(str).map(len)>10]\ndf_ReviewRatingFiltered.shape[0]", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "4846446"}, "execution_count": 12, "metadata": {}}], "execution_count": 12}, {"source": "So, it leaves us with about 4.8 million instances. In fact, these too are also too many for the free tier to model. So, in later stages, I will just be using about 10 percent of it. ", "cell_type": "markdown", "metadata": {}}, {"source": "df_finalRating = df_ReviewRatingFiltered.sample(frac=0.1)\ndf_finalRating = df_finalRating.reset_index(drop=True)\nprint(df_finalRating.shape)\ndf_finalRating.groupby('overall').count()/df_finalRating.shape[0]*100", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "(484645, 2)\n"}, {"output_type": "execute_result", "data": {"text/plain": "         reviewText\noverall            \n1.0        3.432822\n2.0        3.963726\n3.0        9.157837\n4.0       22.458088\n5.0       60.987527", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n    </tr>\n    <tr>\n      <th>overall</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>3.432822</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>3.963726</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>9.157837</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>22.458088</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>60.987527</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 14, "metadata": {}}], "execution_count": 14}, {"source": "The distribution is about the same as the original data set, so we will save this data set as a csv file, later to be used fo rating predictions.", "cell_type": "markdown", "metadata": {}}, {"source": "df_finalRating.to_csv('AmazonBookReviews_Ratings.csv')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 15}, {"source": "#### Preparation for sentiment analysi", "cell_type": "markdown", "metadata": {}}, {"source": "For sentmient analysis, I will remove the middle rating, i.e. 3, and the rating above three will be considered as happy (represented by 1) and less than three as unhappy (represented by 0).", "cell_type": "markdown", "metadata": {}}, {"source": "df_sentiments = df_finalRating[~ (df_finalRating.overall==3)]\ndf_sentiments.loc[df_sentiments.overall < 3.0, 'sentiment'] = 0\ndf_sentiments.loc[df_sentiments.overall > 3.0, 'sentiment'] = 1\ndf_sentiments = df_sentiments[['reviewText','sentiment']]", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/core/indexing.py:357: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[key] = _infer_fill_value(value)\n/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n"}], "execution_count": 21}, {"source": "print(df_sentiments.shape)\ndf_sentiments.groupby('sentiment').count()/df_sentiments.shape[0]*100", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "(440262, 2)\n"}, {"output_type": "execute_result", "data": {"text/plain": "           reviewText\nsentiment            \n0.0          8.142197\n1.0         91.857803", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n    </tr>\n    <tr>\n      <th>sentiment</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0</th>\n      <td>8.142197</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>91.857803</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 22, "metadata": {}}], "execution_count": 22}, {"source": "Now, let's save the dataframe in csv file", "cell_type": "markdown", "metadata": {}}, {"source": "df_sentiments.to_csv('AmazonBookReviews_Sentiment.csv')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 24}, {"source": "So, now we have done the ETL and saved the clean copy of data in csv files. Now, for the data prepaation for the modeling task, modeling and the evaluation will be done in other notebooks.", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark 2.1", "name": "python3-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}